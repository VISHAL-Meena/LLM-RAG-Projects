{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWLMwQzSY_ia"
      },
      "source": [
        "### Youtube video summarizer using wishper and langchain\n",
        "\n",
        "Workflow:\n",
        "- Download the YouTube audio file.\n",
        "- Transcribe the audio using Whisper.\n",
        "- Summarize the transcribed text using LangChain with three different\n",
        " approaches: stuff, refine, and map_reduce.\n",
        "- Adding multiple URLs to DeepLake database, and retrieving information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JBoI2JCZv9C"
      },
      "source": [
        "Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Udrl3YphngVA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"OPENAI_API\"\n",
        "os.environ['ACTIVELOOP_TOKEN']= \"ACTIVELOOP_TOKEN\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "osP_doGgITV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J4j05F2kChU",
        "outputId": "3aacfcc3-2769-4bd9-b3c9-91e78610dd5e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7_2oWZ-XgjL",
        "outputId": "42f377bd-345a-468c-9cfb-be8454e4d3b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-ewxk8xvb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-ewxk8xvb\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.5.2)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802825 sha256=acfadf710491a16784d42338cbc28d7f9d5620d27057567c3f94f772d86ca523\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-33x8esua/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20231117\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.3 rapidfuzz-3.6.1\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-1gC4L4Zzup"
      },
      "outputs": [],
      "source": [
        "from pytube import YouTube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYNSf00sYIyc"
      },
      "outputs": [],
      "source": [
        "link='https://www.youtube.com/watch?v=mBjPyte2ZZo'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhCWNb0vmaJ6"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # object creation using YouTube\n",
        "    yt = YouTube(link)\n",
        "except:\n",
        "    print(\"Connection Error\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apDj4G8ymdFT",
        "outputId": "3effcb78-d53f-4093-afc9-e0222383866e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<Stream: itag=\"18\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.42001E\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">, <Stream: itag=\"22\" mime_type=\"video/mp4\" res=\"720p\" fps=\"30fps\" vcodec=\"avc1.64001F\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">, <Stream: itag=\"136\" mime_type=\"video/mp4\" res=\"720p\" fps=\"30fps\" vcodec=\"avc1.64001f\" progressive=\"False\" type=\"video\">, <Stream: itag=\"135\" mime_type=\"video/mp4\" res=\"480p\" fps=\"30fps\" vcodec=\"avc1.4d401f\" progressive=\"False\" type=\"video\">, <Stream: itag=\"134\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.4d401e\" progressive=\"False\" type=\"video\">, <Stream: itag=\"133\" mime_type=\"video/mp4\" res=\"240p\" fps=\"30fps\" vcodec=\"avc1.4d4015\" progressive=\"False\" type=\"video\">, <Stream: itag=\"160\" mime_type=\"video/mp4\" res=\"144p\" fps=\"30fps\" vcodec=\"avc1.4d400c\" progressive=\"False\" type=\"video\">, <Stream: itag=\"139\" mime_type=\"audio/mp4\" abr=\"48kbps\" acodec=\"mp4a.40.5\" progressive=\"False\" type=\"audio\">, <Stream: itag=\"140\" mime_type=\"audio/mp4\" abr=\"128kbps\" acodec=\"mp4a.40.2\" progressive=\"False\" type=\"audio\">]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "yt.streams.filter(file_extension='mp4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4g6l7qOmnyr"
      },
      "outputs": [],
      "source": [
        "stream = yt.streams.get_by_itag(139)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3p5weO5SmqHU",
        "outputId": "3390259b-e2ee-4d6f-b22b-5af8961f3190"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/letmedown.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "stream.download('',\"lecuninterview.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bEsKNLKmvoJ",
        "outputId": "16dd153f-8e05-495d-ee0d-e255e831849f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:02<00:00, 54.8MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hi, I'm Craig Smith and this is I on A On. This week I talked to Jan LeCoon, one of the seminal figures in deep learning development and a long time proponent of self-supervised learning. Jan spoke about what's missing in large language models and about his new joint embedding predictive architecture which may be a step toward filling that gap. He also talked about his theory of consciousness and the potential for AI systems to someday exhibit the features of consciousness. It's a fascinating conversation that I hope you'll enjoy. Okay, so Jan, it's great to see you again. Good to see you again. I wanted to talk to you about where you've gone with just so supervised learning since last week spoke. In particular, I'm interested in how it relates to large language models because the large language models really came on stream since we spoke. In fact, in your talk about JEPA, which is joint embedding predictive architecture. Thank you. You mentioned that large language models lack a world model. I wanted to talk first about where you've gone with self-supervised learning and where this latest paper stands for your trajectory. But to start, if you could just introduce yourself and we'll go from there. Okay, so my name is Jan Le Ka or Jan Le Kun who want to do it in Gilleswee. And I'm a professor at New York University and at the Guarante Institute in the Centre for Data Science. And I'm also the chief AI scientist at Fair, which is the fundamental AI research lab. That's what Fair stands for. Admetta, Neil, Facebook. So tell me about where you've gone with self-supervised learning, how the joint embedding predictive architecture fits into your research. And then if you could talk about how that relates to what's lacking in large language models. Okay, self-supervised learning has basically brought about a revolution in natural language processing because of their use for pre-training transformer architectures. And the fact that we use transformer architectures for that is somewhat orthogonal to the fact that we use self-supervised learning. But the way those systems are trained is that you take a piece of text, you remove some of the words, you replace them by black markers. And then you train the very large neural net to predict the words that are missing. That's a pre-training phase. And then in the process of training itself to do this, the system learns good representations of text that you can then use as input to its subsequent downstream task, I don't know, translation or hitch-pitch detection or something like that. So that's been a two-year revolution over the last three or four years. And including in sort of very practical applications like every sort of top-performing contact moderation systems on Facebook, Google, YouTube, etc. Use this kind of technique. And there's all kinds of other applications. Now, large language models are partially this, but also the idea that you can train those things to just predict the next word in a text. And if you use that, you can have those system generate text spontaneously. So there's a few issues with this. First of all, those things are what's called generative models in the sense that they predict the words, the information that is missing, words in this case. And the problem with generative models is that it's very difficult to represent uncertain predictions. So in the case of words, it's easy because we just have the system produce essentially what amounts to a score or a probability for every word in the dictionary. And so it cannot tell you if the word missing in a sentence like the blank chases the mouse in the kitchen. It's probably a cat, it could be a dog, but it's probably a cat, right? So you have some distribution of probability over all words in the dictionary. And you can handle uncertainty in the prediction this way. But then what if you want to apply this to, let's say, video, right? So you show a video to the system, you remove some of the frames in that video and you try to predict the frames that are missing. For example, predict what comes next in a video. And that doesn't work. And it doesn't work because it's very difficult to train the system to predict an image or whole image. We have techniques for that for generating images. Before actually predicting good images that could fit in the video, it doesn't work very well. Or if it works, it doesn't produce internal representations that are particularly good for a downstream task like object recognition or something of that type. So attempting to transfer those SSL method that are successful in an LP into the realm of images has not been a big success. It's been somewhat of a success in audio. But really, the only thing that works in the domain of images is those joint embedding architectures where instead of predicting the image, you predict a representation of the image. So you feed let's say one view of a scene to the system, you run it through some neural net that computes representation of it. And then you take a different view of the same scene, you run it through the same network that produces another representation. And you train the system in such a way that those two representations are as close to each other as possible. And the only thing the systems can agree on is the content of the image. So they end up including the content of the image independently of the viewpoint. The difficulty of making this work is to make sure that when you show two different images, it would produce different representations. So to make sure that there are informative about the inputs and your system didn't collapse. It would just produce always the same representation for everything. But that's the reason why the techniques that have been generative architectures have been successful in NLP aren't working so well. In images is their inability to represent complicated uncertainties if you want. So now that's for training a system in SSL to learn representations of data. But what I've been proposing to do in the position paper I published a few months ago is the idea that we should use SSL to get machines to learn predictive world models. So basically to predict whether the world is going to evolve. So predict the continuation of a video, for example. Possibly predict how it's going to evolve as a consequence of an action that an intelligent agent might take. Because if we have such a world model in an agent, the agent being capable of predicting what's going to happen as a consequence of its action will be able to plan complex sequence of actions to arrive at a particular goal. And that's what's missing from all the pretty much all the AI systems that everybody has been working on or has been talking about loudly. Except for a few people who are working on robotics or it's absolutely necessary. So some of the interesting work there comes out of the robotics community, the sort of machine learning and robotics community. Because there you need to have this capability for planning. And the work that you've been doing is it possible to build that into a large language model or is it incompatible with the architecture of large language models? It is compatible with large language models. And in fact, it might solve some of the problems that we're observing with large language models. One point of large language models is that when you use them to generate text, you initialize them with a prompt, right? So you type in an initial segment of a text, which could be in the form of a question or something. And then you hope that it will generate a consistent answer to that text. And the problem with that is that those systems generate text that sounds fine, grammatically but semantically, but sometimes they make various stupid mistakes. And those mistakes are due to two things. The first thing is that to generate that text, they don't really have some sort of objective to then just satisfying the sort of statistical consistency with the prompt that was typed. So there is no way to control the type of answer that will produce. At least no direct way, if you want. That's the first problem. And then the second problem, which is much more acute, is the fact that those large language models have no idea of the underlying reality that language describes. And so there is a limit to how smart it can be and how accurate it can be because they have no experience of the real world, which is really the underlying reality of language. So their understanding of reality is extremely superficial and only contained in whatever is contained in language that they've been trained on. And that's very shallow. Most of human knowledge is completely non-linguistic. It's very difficult for us to realize that's the case, but most of what we learn has nothing to do with language. Language is built on top of a massive amount of background knowledge that we all have in common, that we call common sense. And those machines don't have that, but a cat has it, a dog has it. So we're able to reproduce some of the linguistic abilities of humans without having all the basics that a cat or a dog has about how the world works. And that's why the systems are... I have failures, it's actually... So I think what we would need is an ability for machines to learn how the world works by observation in the manner of babies and infants and young animals accumulate all the background knowledge about the world that constitutes the basis of common sense, if you want. And then use this war model as the tool for being able to plan sequence of actions to arrive at a goal. So sitting goals is also an ability that humans and many animals have, the settings of goals for arriving at an overall goal. And then planning sequences of actions to satisfy those goals. And those my gosh models don't have any of that. They don't have an understanding of the learning world, they don't have a capability of planning for planning, they don't have goals, they can't just send them cells goals, other than through typing a prompt, which is a very weird way. Where are you in your experimentation with this JAPAR architecture? Pretty early. So we have forms of it, simplified form of them that we go joint-time meeting architectures without the P without the predictive. And they work quite well for learning representations of images. So you take an image, you destroy it a little bit, and you train an neural net to produce essentially what I'm also identical representations for those two distorted versions of the same image. And then you have some mechanism for making sure that it produces different representations for different images. And so that works really well. We have simple forms of JAPAR, the predictive version, where the representation of one image is predicted from the representation of the other one. One version of this was actually presented that nerves this is called V-rag-L for local. And it works very well for training the neural net to learn representations that are good for image cementation, for example. But we're still working on a recipe if you want for a system that would be able to learn the properties of the world by watching videos, understanding, for example, very basic concepts, the word is three-dimensional. The system could discover that the world is three-dimensional by being shown video with the moving camera. And the best way to explain how the view of the world changes as the camera moves is that it depicts all the depth that explaining products, motion, et cetera. Once that concept is learned, then the notion of objects and occlusion objects are in front of others, naturally emerges because objects are part of the image that move together with products, motion, at least in animate objects. Animate objects are objects that move by themselves, so there could be also a natural distinction. This ability to spontaneously form the categories. The babies do this at the age of a few months. They have an audio. Without having the names of anything, they know they can tell a car from a bicycle, the chair table, the tree, the chair table. And then on top of this, you can build notions of intuitive physics, the fact that objects that are not supported were full, for example. The babies run this at the age of nine months roughly. It's pretty late. And inertia, six, six, or that type. And then after you've acquired those basic knowledge, background knowledge about how the world works, then you have pretty good ability to predict. And you can also predict perhaps the consequence of your actions when you start acting in the world. And then that gives you the ability to plan. Perhaps it gives you some basis for common sense. So that's the progression that we need to do. We don't know how to do any of this yet. We don't have a good recipe for training a system to predict what's going to happen in the video, for example, within any degree of usefulness. Just for the training portion, how much data would you need? It seems to me, you would need a tremendous amount of data. We need a couple of hours on Instagram or YouTube. That would be enough. Really, the amount of data of raw video data that's available is incredibly large. If you think about let's say five-year-old child, and let's imagine that this five-year-old child can usually analyze visual percept maybe ten times a second. So there's ten friends per second. And if you count how many seconds there are in five-year, it's something like 80 millions. So the child is 800 million friends, right? Or something like that. It's an approximation. It's a billion. It's not that much data. We can have that tomorrow by just recording, like saving a YouTube video or something. So I don't think it's an issue of data. I think it's more an issue of architecture, training paradigm, principles, mathematics, and principles on which to base this. One thing I've said is, if you want to solve that problem, abandon five major pillars of machine learning, one of which is those generator models. And to replace them with those joint embedding architectures, a lot of people envision already convinced of that. Then to abandon the idea of doing probabilistic modeling, so we're not going to be able to predict to represent, usefully, the probability of the continuation of a video from condition on what we have already observed. We have to be less ambitious about or mathematical framework if you want. So I've been advocating for many years to use something called energy based models, which is a weaker form of modeling under a certainty, if you want. Then there is another concept that has been popular for training, joint embedding architectures over the last few years, which had the first paper on in the early 90s, actually, on something called Siamese networks. So it's called contrastive running. And I'm actually advocating against that too. So I used to this idea that once in a while you have to cover up new ideas. And it's going to be very difficult to convince people who are very attached to those ideas to abandon them. But I think it's time for that to happen. Once you've trained one of these networks and you've established a world model, how do you transfer that to the equivalent of a large language model? One of the things that's fascinating about the development of LLMs in the last couple of years is that they're now multi-modal. They're not purely text and language. So how do you combine these two ideas? Or can you or do you need to? Yeah. So there's two or three different questions in that one question. One of them is can we usually transform existing language models? Whose purpose is only to produce text in such a way that they have, they can do the planning and objectives and things like that? The answer is yes. That's probably fairly simple to do. Can we train language model purely on language and expect it to understand the underlying reality? And the answer is no. And in fact, I have a paper on this in a oval places, a philosophy magazine called Noena, which I co-wrote with a card carrying philosopher who is a post document about NYU, where we say that there is a limit to what we can do with this because most of the human knowledge is non-liquistic. And if we only train systems on language, they will have a very superficial understanding of what they're talking about. So if you want systems that are robust and work, we need them to be grounded in reality. And it's an old debate whether AI should be grounded or not. And so the approach that some people have taken at the moment is to basically turn everything, including images and audio into text or something similar to text. So you take an image, you cut it into little squares, you turn those squares into vectors, that's called tokenization, and now an image is just a sequence of tokens. The text is a sequence of words, right? And you do this with everything and you get those multiple systems and they do something, okay? Now clear, that's the right approach long-term, but they do something. I think the ingredients that I'm missing there is the fact that I think if we're dealing with sort of continuous type data like video, we should use the joint embedding architecture, not the generative architectures that large language models currently use. First of all, I don't think we should tokenize them because a lot of it gets lost in translation when we tokenize images and videos. There's a problem also which is that those systems don't scale very well with the number of tokens you feed them with. So it works when you have a text and you need a context to predict the next word that is maybe the 4,000 last words, it's fine, but 4,000 tokens for an image or video is tiny, like you need way more than that and those systems scale horribly with the number of tokens you feed them. So we're going to need to do a lot of new innovations in architectures there and my guess is that we can't do it with generative models, so we'll have to do the joint embedding. How does a computer recognize an image without tokenization? So, convolutional nets, for example, don't tokenize. They take an image as pixels, they extract local features, they detect local motifs on different windows on the image that overlap, and then those motifs get combined into other slightly less local motifs and it's just kind of hierarchy where representations of larger and larger parts of the image are constructed as we go up in the layers, but there's no point where you cut the image into squares and you turn them into individual vectors, it's more sort of progressive. So there's been a bit of a back-and-forth competition between the transformer architectures that tend to rely on this tokenization and convolutional nets, which don't or in different ways. And my guess is that ultimately what would be the best solution is a combination of the two where the first two layers are more like convolutional nets, they exploit the structure of images and videos certainly, and then by the time you get to up to several layers, they are the representation is more object-based and there you have an advantage in using those those transformers, but currently basically the image transformers only have one layer of convolution at the bottom and I think it's a bit of a waste and it doesn't scale very well when you want to apply the entire video. On the timeline, this is all moving very fast. It's moving very fast, yeah. How long do you think before you'll be able to scale this new architecture? It's not just actually coming up with a good recipe that works that would allow us to just plug a large neural net or the smaller on it on YouTube and then learn how the work works by watching in a video. We don't have that recipe, we probably don't have the architecture other than some vague idea, which I call hierarchical japa, but there's a lot of details to figure out that we haven't figured out. This probably failure mode that we haven't yet encountered that we need to find solutions for and so I can't give you a recipe and I can't tell you if we'll come up with a recipe in the next six months, year, two years, five years, ten years, it could be quick or it could be much more difficult than we think, but I think we're on the right path in searching for a solution in that direction. So once we come up with a good recipe, then it will open the door to new breed of AI systems, essentially, that can plan, that can reason and will be much more capable of having some level of common science, perhaps, and have forms of intelligence that are more similar to what we are observing in animals and humans. Your work is inspired by the cognitive processes of the brain and that process of perception and then informing a world model, is that confirmed in neuroscience? It's a hypothesis that is based on some evidence from both neuroscience and cognitive science. So what I showed is a proposal for what's called a cognitive architecture, which is some sort of modular architectures that would be capable of things like planning and reasoning that we observe in capabilities that we observe in animals and humans, and that most cognitive AI systems, except for if your robotics systems don't have. So I think that's important in that respect, but it's more of an inspiration really than a direct copy. I'm interested in understanding the principles behind intelligence, but I would be perfectly happy to come up with some particular that is that uses backpropadial level, but at a higher level kind of does something different from super-vezzoning or something like that, which is why I work on self-super-vezzoning. And so I'm not necessarily convinced that the path towards the satisfying the goal that I was talking about of learning world models, et cetera necessarily goes through finding biological and treat plausible learning procedures. What did you think of the forward forward algorithm, and were you involved in that research? I was not involved, although I've thought about things that are somewhat similar for many decades, but very few of which is actually published. It's in the direct line of a series of work that Jeff has been very passionate about for 40 years of new learning procedures of different types for basically local learning worlds that can train fairly complex neural nets to learn good representations and things like that. So he started with a bosom machine, which was a really interesting concept. That turned out to be somewhat impractical, but very interesting concept that a lot of people started. Backpropadial of course, he and I both had in developing something I worked on also simultaneously with Backprop in the 1980s called target prop, where it's an attempt at making Backpropadial more local by computing a virtual target for every neuron in a large neural net that can be locally optimized. Unfortunately, the way to compute this target is nonlocal. And I haven't worked on this particular type of procedure for a long time, but Yoshra Benjures published a few papers on this over the last 10 years or so. Yoshra Jeff and I, when we started the deep learning conspiracy in the early 2000 to renew the interests of the community and deep learning, we focused largely on forms of kind of local self-supervised learning methods. So things like in GSKs that was focused on restricted bosom machines, Yoshra settled on some single denosing auto encoders, which is the basis for a lot of the large language model type training that we are using today. I was focusing more on what's called sparsato encoders, so there's different ways of doing training a layer if you want in a neural net to learn something useful without being without it being focused on any particular task, so you don't need labeled data. And a lot of that work has been put aside a little bit by the incredible success of just pure supervised learning with this very deep model. We found ways to train very large neural nets with with very many layers with just back prop. And so we put those techniques on the side and Jeff basically is coming back to them. I'm coming back to them in a different form a little bit with this, so the JEPA architecture. And he also had ideas in the past, something called recirculation, a lot of informax methods, which actually the JEPA use these things ideas that are similar. He's a very productive source of ideas that are that sometimes seems out of the left fields and where the community has attention and then doesn't quite figure it out right away. And then it takes a few years for those things to disseminate and sometimes they don't. Just a minute, Jan. Hello. Boe, I'm recording right now. Who? Who? Rasmus? I'll answer when I get back. Yeah, you'll be famous someday. Okay. Okay, great. Thanks very much. Yeah. Bye-bye. Sorry about that. There was a very interesting talk by David Chalmers. At some level it was not a very serious talk because everyone knows as you described earlier that large language models are not reasoning. They don't have common sense. He doesn't claim that they do. No, that's right. But what you're describing with this JEPA architecture, if you could develop a large language model that is based on a world model. You'll be a large language model. You'll be a world model. At first, it would not be based on language. We'll be based on visual perception, maybe audio perception. If you have a machine that can do what a cat does, you don't need a language. A language can be put on top of this. To some extent, language is easy, which is why we have those large language models. And we don't have systems that run how the world work. Yeah. But let's say that you build this world model and you put language on top of it so that you can interrogate it, communicate with it. Does that take you a step toward what Chalmers was talking about? And I don't want to get into the theory of consciousness, but at least an AI model that would exhibit a lot of the features of consciousness. David actually has two different definitions for sentience and consciousness. You can have sentience with our consciousness. Simple animal or sentient, in a sense that they have experience, emotions, and drives and things like that. But they may have the type of consciousness that we think we have, or at least illusion and consciousness, everything we have. So sentience, I think, can be achieved by the type of architecture I propose if we can make them work, which is a big if. And the reason I think that is that what those systems would be able to do is have objectives that they need to satisfy. Think of them as drives. And having the system compute those drives, which would be basically predictions of the outcome of a situation or a sequence of actions that the agent might take. Basically, those would be indistinguishable from emotions. So if you have your own situation, where you can take a sequence of actions to arrive at a result, and the outcomes that you're predicting is terrible results in your destruction. Okay, that creates fear. You try to figure out that is there another sequence of action that I take that would not result in the same outcome. If you make those predictions, but there's a huge uncertainty in the prediction, one of which with probability half, maybe is that you get destroyed, it creates even more fear. And then on the contrary, if the outcome is going to be good, then it's more like elation. So those are long term prediction of outcomes, which systems that use the architecture and proposing, I think, will have. So they will have some level of experience and they will have emotions that will drive the behavior because they would be able to anticipate outcomes. And that has act on them. Now consciousness is a different story. So my full theory of consciousness, which I've talked to David about, thinking it was going to tell me I'm crazy. But he said, no, actually, that overlaps with some pretty common theories of consciousness among philosophers. Is the idea that we have essentially a single world model in our head, somewhere in a prefrontal cortex? And that world model is configurable to the situation we're facing at the moment. And so we're configuring our brain, including our world model, for solving the problem that, you know, satisfying the objective, the recurrently set to ourselves. And because we only have a single world model engine, we can only solve one such task at any one time. This is a characteristic of humans and many animals, which is that when we focus on the task, we can't do anything else. We can do subconscious tasks simultaneously, but we can only do one conscious deliberate task at any one time. And it's because we have a single world model engine. Now, why would evolution build us in a way that we have a single world model engine? There's two reasons for this. One reason is that single world model engine can be configured for the situation at hand, but only the part that changes from one situation to another. And so it can share knowledge between different situations. The physics of the world doesn't change if you are building a table or trying to jump over a river or something. And so your sort of basic knowledge about how the world works doesn't need to be reconfigured. It's only the thing that depends on the situation at hand. So that's one reason. And then the second reason is that if we had multiple models of the world, they would have to be individually less powerful because you have to all fit them within your brain and that's an emitted size. So I think that's probably the reason why we only have one. And so if you have only one world model that needs to be configured for the situation at hand, you need some sort of meta module that configures it, it figures out like what situation am I in, what sub-goals should I set myself, and how should I configure the rest of the my brain to solve that problem. And that module would have to be able to observe the state and capabilities would have to have a model of the rest of itself, of the agent. And that perhaps is something that gives us illusion of consciousness. So I must say this is very speculative. Okay, I'm not saying this is exactly what happens, but it fits with a few things that we know about about consciousness. You were saying that this architecture is inspired by cognitive science or neuroscience. How much do you think your work, Jeff's work, other people's work at the kind of the leading edge of deep learning or machine learning research is informing neuroscience or is it more of the other way around? Certainly in the beginning it was the other way around, but at this point it seems that there's a lot of information that then is reflecting back to those fields. So it's been a bit of a feedback loop. So new concepts in machine learning have driven people in neuroscience and cognitive science to use computational models if you want for whether we're studying and many of my colleagues and my favorite colleagues work on this. They'll feel the computational neuroscience basically is around this. And what we're seeing today is a big influence or rather a wide use of deep learning models such as conditional nets and transformers as models, explanatory models, what goes on in the visual cortex, for example. So the people, you know, for a number of years now who have done FMRI experiments and then showed the same image to a subject in the FMRI machine and to a conventional net and then tried to explain the variance they observe in the activity of various areas of the brain with the activity that is observed in corresponding neural net. And what comes out of the studies is that the notion of motileo hierarchy that we have conventional nets matches the type of hierarchy that we observe in the at least in the ventral pathway of the visual system. So V1 corresponds to the first two layers of the conventional net and then V2 to some of the following layers and V4 more and then the E4 temporal cortex to the top layers. They are the best explanation of each other if you try to do the matching, right? One of my colleagues at Fair Paris, there's a dual affiliation also with Norrespind that academic lab in Paris, has done the same type of experiment using transformer architectures and language models essentially. And observing brain activity of people who are listening to stories and attempting to understand the story so that they can answer questions about the story or give it a summary of it. And there the matching is not that great in the sense that there is some sort of correspondence between the type of activity you observe in those large transformers and the type of activity you observe in the brain but the hierarchy is not nearly as clear and what is clear is that the brain is capable of making much longer term prediction that those language models are capable of today. So that begs the question of what are we missing in terms of architecture and to some extent it's jibes with the idea that the models that we should have should build hierarchical representations of the perceptive, the different levels of abstraction so that the highest level of abstraction are able to make long-term predictions that perhaps are less accurate than the lower level but longer term. We don't need to have that in current models. I had a question I wanted to ask you since our last conversation you have a lot of things going on. You teach, you have your roll-in Facebook, your roll-in I think at CVPR or how do you work on this? You have like three days a week or two hours a day where you're just focused and then are you attinkering with code or with diagrams or is it in iterations with some of your graduates who they're in or is this something where it's kind of always in your mind and you're in the shower and you think yeah that might work. I'm just curious how do you love? Okay so first of all once you understand is that my position at Mita at Fair is not a position of management. I don't manage anything. I'm chief scientist which means I try to inspire others to work on things that I think are promising and I advise several projects that I'm not personally involved in. I work on strategy and orientations and things like this but I don't do data to the management. I'm very thankful that Joelle Pino is doing this for fair and doing very very good job. I'm not very good at it either so it's for you better if I don't if I don't do it so that allows me to spend quite a bit of time on research itself and I don't have a group of engineers and scientists working with me. I have a group of more junior people working with me students and postdocs both at Fair and at NYU both in New York and in Paris and working with students and postdocs is wonderful because they are far less than creative. Many of them have amazing talents in theoretical abilities or implementation abilities or an acrostic things work and so what happens very often is either one of them will come up with an idea that whose results surprise me and so I was thinking that this all wrong and that's the best thing that can happen or sometimes I come up with an idea and turns out to work which is great usually not in the form that I've formulated normally it's there's a lot of contributions that have to be brought to an idea for to make it work and then what's happened also quite a bit in the last few years is I cover with an idea that I'm sure it's going to work and few students and postdocs tried to make it work and they come back to me and said I was sorry it doesn't work and here is the fair and move oh yeah we should have thought about this okay so here's a new idea to get around this problem so for example several years ago I was advocating for the use of generative models with latent variables to handle the uncertainty and a completely changed my mind about this now advocating for those joint evading architecture that do not actually predict and I was I more or less invented those contrasting methods that a lot of people are talking about and using at this point and I'm advocating against them now in favor of those methods such as v-creg or about the twins that basically instead of using contrasting methods can try to maximize the information content of representations and that idea of information maximization had known about for decades because Jeff was working on this in the 1980s when I was a postdoc with him and he abandoned the idea pretty much he had a couple papers with one of his student called Sue Becker in the early 90s that show that he could work but only in sort of small dimension and he pretty much abandoned it and the reason he abandoned it is because of a major flaw with those methods due to the fact that we don't have any good measures of information content or the measures that we had are upper bound not lower bound so we can't try to maximize information content very well and so I never thought about those that those methods could ever work because of my experience with with that and and one of my postdocs, Defendede actually kind of revised the idea and surely that it worked that was the bottom of a twins paper and so we changed my mind and so now that we had a new tool, information maximization applied to the joint embedding architectures and came up with an improvement of it called v-creg and and now we're working on that but there are other ideas we're working on to solve the same problem with other groups of people at the moment which probably will come up in the next few months so we don't again we don't have a perfect recipe yet and we're looking for one and hopefully one of the things that we are working on with stick. Yeah are you coding models and then training them and running them or are you conceptualizing and turning it over to someone else? So it's mostly conceptualizing and mostly letting the students and postdocs doing the implementation although I do a little bit of coding myself but not enough to my taste I wish I could do more. I have a lot of postdocs and students and so I have to devote sufficient amount of my time to interact with them and then leave them some breathing room to do the work they do best and so it's an interesting question because that question was asked to Jeff to start right yeah and he said he was using MATLAB and and he said you have to do those things yourself because if you give a project to a student and a project come back saying it doesn't work you don't know if it's because there is a conceptual problem with the idea or whether it's just some stupid detail that wasn't done right and when I'm facing with this that's when I start looking at the code and perhaps experimenting with it myself or I get multiple students to work on them to collaborate on the project so that if one makes an error perhaps the other one will detect what it is. I love coding I just don't do as much as I like it. This JEPA or the forward forward things have moved so quickly you think back to when the transformers were introduced or at least the attention mechanism and that kind of shifted the field it's difficult for an outsider to judge when I hear the JEPA talk is this one of those moments that wow this idea is going to transform the field or have you been through many of these moments and they contribute to some extent but they're not the answer to ship the paradigm. It's hard to tell at first but whenever I kind of keep pursuing an idea and promote it it's because I have a good hunch that they're going to have a relatively big impact and it was easy for me to do before I was as famous as I am now because I wasn't listened to that much so I could make some claim and now I have to be careful what I claim because a lot of people listen to me yeah and it's the same issue with JEPA so JEPA for example a few years ago was promoting this idea of capsules. Yeah and everybody was thinking this is going to be like a big thing a lot of people started working on it. It turns out it's very hard to make it work and it didn't have the impact that many people study would have including JEPA and it turned out to be limited by implementation issues and stuff like that. The underlying idea behind it is good but like very often the practical side of it there was the case also with Wilson machines they're conceptually super interesting. They just don't work that well. They don't scale very well. They're very slow to train because actually it's a very interesting idea that everybody should know about. So there's a lot of those ideas that are conceptually that allow us there are some mental objects that allow us to think differently about what we do but they may not actually have that much practical impact. For forward we don't know yet. Okay it could be like the weak sleep algorithm that JEP talked about 20 years ago or something or it could be the new back prop we don't know. Yeah or the new target prop which is interesting but not really mainstream because it has some advantages in some situations but it's not it brings you like an improved performance on some standard benchmark that people are interested in so it doesn't have the wider deal perhaps. So it's hard to figure out but what I can tell you is that if we figure out how to train one of those JEPA start architecture from video and the representations that it learns are good and the predictive model that he learns are good. This is going to open the door to a new breed of AI systems. I have no no doubt about that. It's exciting the speed at which things have been moving in particular in the last three years about about transformers and the history of transformers. Once you only say about this is that we see the most visible progress but we don't realize that how much of a history there was behind it and even the people who actually come up with some of those ideas don't realize that their ideas actually had roots in other things. For example back in the 90s people were already working on things that we could now call mixer of experts and also multiplicative interactions which at the time were called the semi-pi networks or things like that. So it's the idea that instead of having two variables that you add together with weights you multiply them and then you have a way for you have weights before you multiply it doesn't matter. This idea goes back every long time since the 1980s and then you had ideas of linearly combining multiple inputs with weights that are between 0 and 1 and sum to 1 and are did that dependent. So now we call this attention but this is a circuit that we use in mixer of expert models back in the early 90s also. So that's how it is called. Then there were ideas of neural networks that have a separate module for computation and memory that's the two separate modules. So one module that is a classical neural net and the output of that module would be an address into an associative memory that itself would be a different type of neural net and those different types of neural net associative memories use what we now call attention. So they compute the similarity or the product between a query vector and a bunch of key vectors and then they normalize them so they sum to 1 and then the output of the memory is weighted sum of the value value vectors. There was a series of papers by my colleagues in the early days of fair actually in 2014-15 one called memory network one called end-to-end memory network one called the stack augmented memory network and another one called the key value memory network and then a whole bunch of things and those use those associative memories that basically are the basic modules that are used inside the transformers and then attention mechanism like this were popularized in around 2015 by a paper from the Yoshua Ventures group at Miller and demonstrated that they are extremely the powerful for doing things like translation language translation in NLP and that really started the craze on attention. And so you command all those ideas and you get a transformer that uses something called self-attention where the input tokens are used both as queries and keys in a associative memory very much like a memory network and then you use this as a layer if you want you put several of those in a layer and then you stack those layers and that's what a transformer is. The affiliation is not obvious but there is one. Those ideas have been around and people have been talking about it and with similar work also around 2015-16 and from deep mine called neural turning machine or a differential neural computer. Those ideas that you have a separate module for computation and another one for memory. There's a paper by SEP or HIGHTER and his group also on neural nets that have separate memory associative memory type system. They are the same type of things. I think this idea is very powerful. A big advantage of transformers is that the same way conventional nets are equivalent to shift. So to shift the input of a conventional net the output also shifts but otherwise they are unchanged. If a transformer if you permute the input tokens the output tokens get permuted the same way but are otherwise unchanged. So, com nets are equivalent to shifts. Transformers are equivalent to permutation and with a combination of the two it's great. She's why I think the combination of a combination of a low level and transformer at the top I think for natural input data like image and video is a written combination. Is there a combinatorial effect as the field progresses all of these ideas create a cascade of new ideas? Is that why the field is speeding up? It's not the only reason. There's a number of reasons. One of the reasons is that you build on each other's ideas and etc. which of course is the hallmark of science in general also art. But there is a number of characteristics. I think that helps that to a large extent. The one in particular is the fact that most research work in this area now comes with code that other people can use and build a pump. The habit of distributing your code in a source I think is an enormous contributor to the acceleration of progress. The other one is the availability of the most sophisticated tools like PyTorch for example or TensorFlow or Jax or things like that where researchers can build on top of each other's code base basically to come up with really complex concepts. And all of this is committed by the fact that some of the main contributors that are from industry to those ideas don't seem to be too obsessive-compulsive about IP protection. So meta and in particular is very open. We may occasionally find patents but we're not going to see you for infringing them unless you sue us. Google has a similar policy. You don't see this much from companies that tend to be a little more secretive about their research like Apple and Amazon. But although I just talked to Sammy Benio he's trying to implement that openness. More power to him. Good luck. It's a culture change for a company like Apple so this is not a battle I want to fight but if you can win it like good for him. Yeah. It's difficult. It's a difficult battle. Also I think another contributor is that there are real practical commercial applications of all of this. They're not just imagined they are real. And so that creates a market and that increases the size of the community. And so that creates more appeal for new ideas. More more outlets if you want for new ideas. Do you think that this hockey stick and curve is going to continue for a while or do you think we'll hit a plateau then? It's difficult to say nothing looks more like an exponential that the beginning of a sigmoid. So every natural process has a saturated at some point. The question is when? And I don't see any obvious wall that is being hit by AI research at the moment. It's quite the opposite. It seems to be an acceleration in fact of progress. And there's no question that we need new concepts and new ideas. In fact that's the purpose of my research at the moment because I think there are limitations to current approaches. So this is not to say that we just need to scale up deep learning and turn the crank and we'll get to a human level of intelligence. I don't believe that. I don't believe that it's just a matter of making reinforcement learning more efficient. I don't think that's possible with the current way reinforcement learning is formulated. And we're not going to get there with supervised learning either. I think we definitely need new innovative concepts. But I don't see any slowdown yet. I don't see any people turning away from AI saying it's obviously not going to work. Despite there is screams of various critiques. But to some extent at the moment are fighting a re-art guard battle because they plan to flag this and you're never going to be able to do this. And then turns out you can do this. So they plan to flag a little further down. And now you're not going to be able to do this. So, Tidey. Yeah. Okay. My last question. Are you still doing music? Hi, I'm. And are you still building instruments? Are we building instruments? Electronic wind instruments? Yes. I'm in process of designing a new one. Wow. Yeah. Okay. Maybe I think I said this last time. Maybe I could get some recordings and put them into the podcast or something. Right. I probably told you not such a great performer. I'm probably better at conceptualizing and building those instruments and playing them. But yeah, it's possible. That's it for this episode. I want to thank Jan for his time. If you want to read a transcript of today's conversation, you can find one on our website, iOnAI, that's e-i-i-n-o-n.ai. Feel free to drop us a line with comments or suggestions at Craig at iOnAI, that's CR-A-I-G, at e-i-e-i-n-o-n.ai. And remember, the singularity may not be near, but AI is about to change your world, so pay attention.\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(\"lecuninterview.mp4\")\n",
        "print(result['text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open ('text.txt', 'w') as file:\n",
        "    file.write(result['text'])"
      ],
      "metadata": {
        "id": "8923GtxZ0D-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEvpnfJgbH5_"
      },
      "outputs": [],
      "source": [
        "# import yt_dlp\n",
        "\n",
        "# def download_mp4_from_youtube(url):\n",
        "#     # Set the options for the download\n",
        "#     filename = 'lecuninterview.mp4'\n",
        "#     ydl_opts = {\n",
        "#         'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',\n",
        "#         'outtmpl': filename,\n",
        "#         'quiet': True,\n",
        "#     }\n",
        "\n",
        "#     # Download the video file\n",
        "#     with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "#         result = ydl.extract_info(url, download=True)\n",
        "\n",
        "# url = \"https://youtu.be/r03JffgFK1Y?si=blrFeUIF9llmq0ZI\"\n",
        "# download_mp4_from_youtube(url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.mapreduce import MapReduceChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.summarize import load_summarize_chain"
      ],
      "metadata": {
        "id": "SiefXfdS0KQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-instruct\",temperature=0)"
      ],
      "metadata": {
        "id": "LVpsu9pA549B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
        ")"
      ],
      "metadata": {
        "id": "Be0YEpgsyoAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "with open('text.txt') as f:\n",
        "    text = f.read()\n",
        "\n",
        "texts = text_splitter.split_text(text)\n",
        "docs = [Document(page_content=t) for t in texts[:4]]"
      ],
      "metadata": {
        "id": "xbPK858Uyn53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import textwrap\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "\n",
        "output_summary = chain.run(docs)\n",
        "wrapped_text = textwrap.fill(output_summary, width=100)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9c3FJWPyn28",
        "outputId": "e865e4a0-b287-46cb-d5ca-3656bba8f519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-4WvbxdPrEP0u1TuVPWHwoZwd on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-4WvbxdPrEP0u1TuVPWHwoZwd on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-4WvbxdPrEP0u1TuVPWHwoZwd on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-4WvbxdPrEP0u1TuVPWHwoZwd on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-4WvbxdPrEP0u1TuVPWHwoZwd on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-4WvbxdPrEP0u1TuVPWHwoZwd on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-4WvbxdPrEP0u1TuVPWHwoZwd on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-4WvbxdPrEP0u1TuVPWHwoZwd on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jan LeCoon, a prominent figure in deep learning development and advocate for self-supervised\n",
            "learning, discusses the limitations of large language models and introduces his new joint embedding\n",
            "predictive architecture. He emphasizes the impact of self-supervised learning on natural language\n",
            "processing and the importance of transformer architectures. Self-supervised learning involves\n",
            "training neural networks to predict missing words in text, revolutionizing content moderation\n",
            "systems. However, generative models struggle to represent uncertain predictions, posing a challenge\n",
            "when applied to other forms of media like video.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( chain.llm_chain.prompt.template )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBYBBN_pzacv",
        "outputId": "e762187a-587f-426a-c199-fd96747b863d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a concise summary of the following:\n",
            "\n",
            "\n",
            "\"{text}\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Write a concise bullet point summary of the following:\n",
        "\n",
        "\n",
        "{text}\n",
        "\n",
        "\n",
        "CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
        "\n",
        "BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template,\n",
        "                        input_variables=[\"text\"])"
      ],
      "metadata": {
        "id": "LL_WHxvtz3Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"stuff\",\n",
        "                             prompt=BULLET_POINT_PROMPT)\n",
        "\n",
        "output_summary = chain.run(docs)\n",
        "\n",
        "wrapped_text = textwrap.fill(output_summary,\n",
        "                             width=1000,\n",
        "                             break_long_words=False,\n",
        "                             replace_whitespace=False)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wo6XRTT6l5k",
        "outputId": "cadb5c27-7605-48cd-b81e-4368b4d50cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Craig Smith interviews Jan LeCoon, a prominent figure in deep learning development and advocate for self-supervised learning.\n",
            "- Jan discusses the limitations of large language models and introduces his joint embedding predictive architecture (JEPA) as a potential solution.\n",
            "- Jan also shares his theory of consciousness and the potential for AI systems to exhibit conscious features.\n",
            "- Self-supervised learning has revolutionized natural language processing by pre-training transformer architectures using text prediction.\n",
            "- Large language models can generate text spontaneously, but struggle with representing uncertain predictions.\n",
            "- Jan's JEPA aims to address this limitation and expand the application of self-supervised learning to other domains like video analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
        "\n",
        "output_summary = chain.run(docs)\n",
        "wrapped_text = textwrap.fill(output_summary, width=100)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9ohmb7wGsd4",
        "outputId": "3de6def0-a0d2-4173-a83e-e87556ab74b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-4WvbxdPrEP0u1TuVPWHwoZwd on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-4WvbxdPrEP0u1TuVPWHwoZwd on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-4WvbxdPrEP0u1TuVPWHwoZwd on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-4WvbxdPrEP0u1TuVPWHwoZwd on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jan LeCoon, a prominent figure in deep learning development and advocate for self-supervised\n",
            "learning, discusses the significance of self-supervised learning in revolutionizing natural language\n",
            "processing and the use of transformer architectures. He explains the process of training large\n",
            "neural nets to predict missing words in text, which results in the system learning good\n",
            "representations of text that can be used for various downstream tasks. Jan also introduces his new\n",
            "joint embedding predictive architecture, which addresses the limitations of large language models.\n",
            "Additionally, he shares that self-supervised learning has been widely adopted in practical\n",
            "applications such as contact moderation systems on platforms like Facebook, Google, and YouTube. The\n",
            "limitations of generative models in representing uncertain predictions are highlighted, particularly\n",
            "in the case of applying self-supervised learning to video analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import yt_dlp\n",
        "\n",
        "# def download_mp4_from_youtube(urls, job_id):\n",
        "#     # This will hold the titles and authors of each downloaded video\n",
        "#     video_info = []\n",
        "\n",
        "#     for i, url in enumerate(urls):\n",
        "#         # Set the options for the download\n",
        "#         file_temp = f'./{job_id}_{i}.mp4'\n",
        "#         ydl_opts = {\n",
        "#             'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',\n",
        "#             'outtmpl': file_temp,\n",
        "#             'quiet': True,\n",
        "#         }\n",
        "\n",
        "#         # Download the video file\n",
        "#         with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "#             result = ydl.extract_info(url, download=True)\n",
        "#             title = result.get('title', \"\")\n",
        "#             author = result.get('uploader', \"\")\n",
        "\n",
        "#         # Add the title and author to our list\n",
        "#         video_info.append((file_temp, title, author))\n",
        "\n",
        "#     return video_info\n",
        "\n",
        "# urls=[\"https://www.youtube.com/watch?v=mBjPyte2ZZo&t=78s\",\n",
        "#     \"https://www.youtube.com/watch?v=cjs7QKJNVYM\",]\n",
        "# vides_details = download_mp4_from_youtube(urls, 1)"
      ],
      "metadata": {
        "id": "FDbWhpSeG0IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import DeepLake\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
        "\n",
        "# create Deep Lake dataset\n",
        "# TODO: use your organization id here. (by default, org id is your username)\n",
        "my_activeloop_org_id = \"vishalug3016\"\n",
        "my_activeloop_dataset_name = \"langchain_course_youtube_summarizer\"\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
        "\n",
        "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
        "db.add_documents(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI26PvyCH0Ul",
        "outputId": "d8e73a08-296b-4fcc-e899-c79e6a0d912d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating 4 embeddings in 1 batches of size 4:: 100%|██████████| 1/1 [00:06<00:00,  6.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://vishalug3016/langchain_course_youtube_summarizer', tensors=['text', 'metadata', 'embedding', 'id'])\n",
            "\n",
            "  tensor      htype      shape     dtype  compression\n",
            "  -------    -------    -------   -------  ------- \n",
            "   text       text      (4, 1)      str     None   \n",
            " metadata     json      (4, 1)      str     None   \n",
            " embedding  embedding  (4, 1536)  float32   None   \n",
            "    id        text      (4, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['63ebfa9a-c2e5-11ee-ba24-0242ac1c000c',\n",
              " '63ebfc3e-c2e5-11ee-ba24-0242ac1c000c',\n",
              " '63ebfd10-c2e5-11ee-ba24-0242ac1c000c',\n",
              " '63ebfdce-c2e5-11ee-ba24-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever()\n",
        "retriever.search_kwargs['distance_metric'] = 'cos'\n",
        "retriever.search_kwargs['k'] = 4"
      ],
      "metadata": {
        "id": "uz_8T20yIks7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template = \"\"\"Use the following pieces of transcripts from a video to answer the question in bullet points and summarized. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Summarized answer in bullter points:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "BwZyirtnI9Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "qa = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                 chain_type=\"stuff\",\n",
        "                                 retriever=retriever,\n",
        "                                 chain_type_kwargs=chain_type_kwargs)\n",
        "\n",
        "print( qa.run(\"Summarize the mentions of google according to their AI program\") )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxlepkijJE2Q",
        "outputId": "afdcec29-4444-4960-f39b-ca3bd374f354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Google is mentioned as one of the companies that uses self-supervised learning in their top-performing contact moderation systems.\n",
            "- The speaker mentions that large language models, including those used by Google, lack the ability to represent uncertain predictions.\n",
            "- There is no specific mention of Google's AI program in relation to self-supervised learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "povfJha7JMdR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}